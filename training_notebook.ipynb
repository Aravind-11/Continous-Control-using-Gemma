{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08538f1d-a569-4c4e-8c5c-f3c7cea44f37",
      "metadata": {
        "id": "08538f1d-a569-4c4e-8c5c-f3c7cea44f37"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from q_learning_agent import QLearningAgent\n",
        "\n",
        "def train_agent_with_plotting():\n",
        "    \"\"\"Train a Q-learning agent from scratch and plot its learning performance\"\"\"\n",
        "\n",
        "    print(\"Creating a new FrozenLake environment...\")\n",
        "    env = gym.make('FrozenLake-v1', is_slippery=True)\n",
        "\n",
        "    # Initialize a fresh agent\n",
        "    print(\"Initializing a new Q-learning agent...\")\n",
        "    q_agent = QLearningAgent(env)\n",
        "\n",
        "    # Training parameters\n",
        "    num_episodes = 50000\n",
        "    eval_interval = 500\n",
        "    rewards = []\n",
        "    success_rates = []\n",
        "\n",
        "    print(f\"Starting training for {num_episodes} episodes...\")\n",
        "\n",
        "    # Main training loop\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            # Choose an action using epsilon-greedy policy\n",
        "            action = q_agent.choose_action(state)\n",
        "\n",
        "            # Take step in environment with explicit boolean conversion\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = bool(terminated) or bool(truncated)\n",
        "\n",
        "            # Update Q-table\n",
        "            q_agent.update(state, action, reward, next_state, done)\n",
        "\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "        # Record reward\n",
        "        rewards.append(total_reward)\n",
        "\n",
        "        # Evaluate periodically\n",
        "        if (episode + 1) % eval_interval == 0:\n",
        "            # Evaluate current policy\n",
        "            success_count = 0\n",
        "            eval_episodes = 100\n",
        "\n",
        "            for _ in range(eval_episodes):\n",
        "                eval_state, _ = env.reset()\n",
        "                eval_done = False\n",
        "\n",
        "                while not eval_done:\n",
        "                    # Use greedy policy for evaluation\n",
        "                    eval_action = q_agent.get_optimal_action(eval_state)\n",
        "                    eval_next_state, eval_reward, eval_terminated, eval_truncated, _ = env.step(eval_action)\n",
        "                    eval_done = bool(eval_terminated) or bool(eval_truncated)\n",
        "                    eval_state = eval_next_state\n",
        "\n",
        "                    # Check if reached goal\n",
        "                    if eval_reward > 0:\n",
        "                        success_count += 1\n",
        "                        break\n",
        "\n",
        "            success_rate = success_count / eval_episodes\n",
        "            success_rates.append(success_rate)\n",
        "\n",
        "            print(f\"Episode {episode+1}/{num_episodes}, Success rate: {success_rate:.4f}, Exploration rate: {q_agent.exploration_rate:.4f}\")\n",
        "\n",
        "    # Save the trained Q-table\n",
        "    q_agent.save(\"q_table_new.pkl\")\n",
        "    print(\"Training complete! Q-table saved to 'q_table_new.pkl'\")\n",
        "\n",
        "    # Plot the learning curves\n",
        "    plot_learning_curves(rewards, success_rates, eval_interval)\n",
        "\n",
        "    return q_agent\n",
        "\n",
        "def plot_learning_curves(rewards, success_rates, eval_interval):\n",
        "    \"\"\"Create plots of the learning performance\"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
        "\n",
        "    # Plot 1: Smoothed rewards\n",
        "    window_size = 100\n",
        "    smoothed_rewards = np.convolve(rewards, np.ones(window_size)/window_size, mode='valid')\n",
        "\n",
        "    ax1.plot(range(window_size-1, len(rewards)), smoothed_rewards)\n",
        "    ax1.set_xlabel('Episode')\n",
        "    ax1.set_ylabel('Smoothed Reward (100-episode window)')\n",
        "    ax1.set_title('Training Rewards')\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Plot 2: Success rates\n",
        "    episodes = [(i+1)*eval_interval for i in range(len(success_rates))]\n",
        "    ax2.plot(episodes, success_rates, marker='o', linestyle='-')\n",
        "    ax2.set_xlabel('Episode')\n",
        "    ax2.set_ylabel('Success Rate')\n",
        "    ax2.set_title('Policy Evaluation (100 test episodes)')\n",
        "    ax2.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('learning_performance.png')\n",
        "    print(\"Learning curves saved to 'learning_performance.png'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Train agent from scratch and plot learning curve\n",
        "    agent = train_agent_with_plotting()\n",
        "\n",
        "    # Final evaluation\n",
        "    env = gym.make('FrozenLake-v1', is_slippery=True)\n",
        "    success_count = 0\n",
        "    final_eval_episodes = 1000\n",
        "\n",
        "    print(f\"Running final evaluation over {final_eval_episodes} episodes...\")\n",
        "\n",
        "    for i in range(final_eval_episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.get_optimal_action(state)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = bool(terminated) or bool(truncated)\n",
        "            state = next_state\n",
        "\n",
        "            if reward > 0:\n",
        "                success_count += 1\n",
        "                break\n",
        "\n",
        "    final_success_rate = success_count / final_eval_episodes\n",
        "    print(f\"Final Success Rate: {final_success_rate:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e0acc2b-5b55-430b-8787-5ae19887f2a4",
      "metadata": {
        "id": "6e0acc2b-5b55-430b-8787-5ae19887f2a4"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from tqdm import tqdm\n",
        "from q_learning_agent import QLearningAgent\n",
        "import csv\n",
        "\n",
        "def collect_successful_q_values(q_agent, num_evaluation_episodes=10000, is_slippery=True):\n",
        "    \"\"\"\n",
        "    Collect Q-values from all successful episodes during evaluation.\n",
        "\n",
        "    Args:\n",
        "        q_agent: Trained Q-learning agent\n",
        "        num_evaluation_episodes: Number of episodes to evaluate\n",
        "        is_slippery: Whether the frozen lake is slippery\n",
        "\n",
        "    Returns:\n",
        "        successful_episodes: List of successful episodes, each containing states and Q-values\n",
        "    \"\"\"\n",
        "    env = gym.make('FrozenLake-v1', is_slippery=is_slippery)\n",
        "    successful_episodes = []\n",
        "\n",
        "    for _ in tqdm(range(num_evaluation_episodes), desc=\"Evaluating Episodes\"):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        episode_states = []\n",
        "        episode_q_values = []\n",
        "\n",
        "        while not done:\n",
        "            # Record state and Q-values\n",
        "            episode_states.append(state)\n",
        "            episode_q_values.append(q_agent.get_q_values(state).copy())\n",
        "\n",
        "            # Choose and take action\n",
        "            action = q_agent.get_optimal_action(state)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            state = next_state\n",
        "\n",
        "            # Check if episode was successful\n",
        "            if terminated and reward > 0:\n",
        "                # Store this successful episode\n",
        "                successful_episodes.append({\n",
        "                    \"states\": episode_states,\n",
        "                    \"q_values\": episode_q_values\n",
        "                })\n",
        "                break\n",
        "\n",
        "    env.close()\n",
        "    print(f\"Collected {len(successful_episodes)} successful episodes out of {num_evaluation_episodes}\")\n",
        "    return successful_episodes\n",
        "\n",
        "def create_batched_q_values(successful_episodes, batch_size=100):\n",
        "    \"\"\"\n",
        "    Create batches of averaged Q-values from successful episodes.\n",
        "\n",
        "    Args:\n",
        "        successful_episodes: List of successful episodes\n",
        "        batch_size: Number of episodes to average per batch\n",
        "\n",
        "    Returns:\n",
        "        batched_avg_q_values: List of dictionaries mapping states to average Q-values for each batch\n",
        "    \"\"\"\n",
        "    batched_avg_q_values = []\n",
        "\n",
        "    # Process episodes in batches\n",
        "    num_batches = len(successful_episodes) // batch_size\n",
        "    if num_batches == 0 and len(successful_episodes) > 0:\n",
        "        num_batches = 1  # At least one batch if we have successful episodes\n",
        "\n",
        "    for batch_idx in range(num_batches):\n",
        "        start_idx = batch_idx * batch_size\n",
        "        end_idx = min(start_idx + batch_size, len(successful_episodes))\n",
        "\n",
        "        batch_episodes = successful_episodes[start_idx:end_idx]\n",
        "\n",
        "        # Collect all Q-values by state for this batch\n",
        "        state_q_values = {}\n",
        "        for episode in batch_episodes:\n",
        "            for s, q_vals in zip(episode[\"states\"], episode[\"q_values\"]):\n",
        "                if s not in state_q_values:\n",
        "                    state_q_values[s] = []\n",
        "                state_q_values[s].append(q_vals)\n",
        "\n",
        "        # Average Q-values for each state in this batch\n",
        "        avg_q_values = {}\n",
        "        for state, q_values_list in state_q_values.items():\n",
        "            avg_q_values[state] = np.mean(q_values_list, axis=0)\n",
        "\n",
        "        batched_avg_q_values.append(avg_q_values)\n",
        "        print(f\"Created batch {batch_idx+1}/{num_batches} with {len(batch_episodes)} episodes\")\n",
        "\n",
        "    return batched_avg_q_values\n",
        "\n",
        "def generate_expert_demonstrations_with_batched_q_values(q_agent, batched_avg_q_values,\n",
        "                                                         demos_per_batch=100, is_slippery=True):\n",
        "    \"\"\"\n",
        "    Generate expert demonstrations using a trained Q-learning agent and batched average Q-values.\n",
        "\n",
        "    Args:\n",
        "        q_agent: Trained Q-learning agent\n",
        "        batched_avg_q_values: List of dictionaries mapping states to average Q-values for each batch\n",
        "        demos_per_batch: Number of demonstrations to generate per batch\n",
        "        is_slippery: Whether the frozen lake is slippery\n",
        "\n",
        "    Returns:\n",
        "        all_demos: List of demonstration dictionaries\n",
        "    \"\"\"\n",
        "    env = gym.make('FrozenLake-v1', is_slippery=is_slippery, render_mode=\"ansi\")\n",
        "    all_demos = []\n",
        "\n",
        "    # Action mapping\n",
        "    action_map = {0: \"LEFT\", 1: \"DOWN\", 2: \"RIGHT\", 3: \"UP\"}\n",
        "\n",
        "    # Generate demonstrations for each batch\n",
        "    for batch_idx, avg_q_values in enumerate(batched_avg_q_values):\n",
        "        batch_demos = []\n",
        "\n",
        "        for _ in tqdm(range(demos_per_batch),\n",
        "                     desc=f\"Generating Demos for Batch {batch_idx+1}/{len(batched_avg_q_values)}\"):\n",
        "            state, _ = env.reset()\n",
        "            done = False\n",
        "\n",
        "            while not done:\n",
        "                # Get state representation\n",
        "                state_str = env.render()\n",
        "\n",
        "                # Get expert action from Q-table\n",
        "                action = q_agent.get_optimal_action(state)\n",
        "                action_text = action_map[action]\n",
        "\n",
        "                # Get Q-values from the averaged batch\n",
        "                if state in avg_q_values:\n",
        "                    q_values = avg_q_values[state]\n",
        "                else:\n",
        "                    # Fallback to agent's Q-values if state not in the averaged batch\n",
        "                    q_values = q_agent.get_q_values(state)\n",
        "\n",
        "                # Take action in environment\n",
        "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "                done = terminated or truncated\n",
        "\n",
        "                # Create demo example\n",
        "                demo = {\n",
        "                    \"state\": state,\n",
        "                    \"state_str\": state_str,\n",
        "                    \"action\": action,\n",
        "                    \"action_text\": action_text,\n",
        "                    \"q_values\": q_values.tolist(),\n",
        "                    \"reward\": reward,\n",
        "                    \"next_state\": next_state,\n",
        "                    \"done\": done,\n",
        "                    \"batch_idx\": batch_idx  # Track which batch this demo came from\n",
        "                }\n",
        "\n",
        "                batch_demos.append(demo)\n",
        "                state = next_state\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "        all_demos.extend(batch_demos)\n",
        "\n",
        "    env.close()\n",
        "    return all_demos\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load a trained expert agent\n",
        "    env = gym.make('FrozenLake-v1', is_slippery=True)\n",
        "    q_agent = QLearningAgent(env)\n",
        "\n",
        "    # Try to load the saved expert Q-table\n",
        "    try:\n",
        "        q_agent.load(\"q_table_new.pkl\")\n",
        "        print(\"Loaded expert Q-table.\")\n",
        "\n",
        "        # Verify it's an expert\n",
        "        success_rate = q_agent.evaluate(num_episodes=100)\n",
        "        print(f\"Expert agent success rate: {success_rate:.2f}\")\n",
        "\n",
        "        if success_rate < 0.7:\n",
        "            print(\"WARNING: Loaded agent does not meet expertise threshold!\")\n",
        "            print(\"Run train_expert_agent.py first to generate an expert agent.\")\n",
        "            exit(1)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"No expert Q-table found. Run train_expert_agent.py first.\")\n",
        "        exit(1)\n",
        "\n",
        "    # Steps 1-3 remain the same\n",
        "    print(\"Collecting Q-values from successful episodes...\")\n",
        "    successful_episodes = collect_successful_q_values(q_agent, num_evaluation_episodes=10000)\n",
        "\n",
        "    print(\"Creating batched average Q-values...\")\n",
        "    batched_avg_q_values = create_batched_q_values(successful_episodes, batch_size=100)\n",
        "\n",
        "    print(\"Generating expert demonstrations with batched average Q-values...\")\n",
        "    all_demos = generate_expert_demonstrations_with_batched_q_values(\n",
        "        q_agent, batched_avg_q_values, demos_per_batch=100)\n",
        "\n",
        "    # Save demonstrations to CSV\n",
        "    csv_filename = \"expert_demos_batched_avg_q.csv\"\n",
        "    fieldnames = [\n",
        "        'state', 'state_str', 'action', 'action_text',\n",
        "        'q_value_left', 'q_value_down', 'q_value_right', 'q_value_up',\n",
        "        'reward', 'next_state', 'done', 'batch_idx'\n",
        "    ]\n",
        "\n",
        "    print(f\"Saving demonstrations to {csv_filename}...\")\n",
        "    with open(csv_filename, 'w', newline='') as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "\n",
        "        for demo in all_demos:\n",
        "            # Create a flattened version of the demo dictionary\n",
        "            flat_demo = {\n",
        "                'state': demo['state'],\n",
        "                'state_str': demo['state_str'].replace('\\n', '|'),  # Replace newlines with pipe for CSV\n",
        "                'action': demo['action'],\n",
        "                'action_text': demo['action_text'],\n",
        "                'q_value_left': demo['q_values'][0],\n",
        "                'q_value_down': demo['q_values'][1],\n",
        "                'q_value_right': demo['q_values'][2],\n",
        "                'q_value_up': demo['q_values'][3],\n",
        "                'reward': demo['reward'],\n",
        "                'next_state': demo['next_state'],\n",
        "                'done': demo['done'],\n",
        "                'batch_idx': demo['batch_idx']\n",
        "            }\n",
        "            writer.writerow(flat_demo)\n",
        "\n",
        "    print(f\"Generated and saved {len(all_demos)} expert demonstrations to CSV\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ca91920-66f9-462b-96da-f3f07b644c22",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "5ca91920-66f9-462b-96da-f3f07b644c22",
        "outputId": "4436015e-4103-49a0-f2f8-2cdc634a80d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error importing torch or verifiers: No module named 'verifiers.trainers'\n",
            "This functionality requires PyTorch and the verifiers package.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Dict, List, Any, Optional, Union, Tuple\n",
        "\n",
        "class XMLParser:\n",
        "    \"\"\"Parser for extracting content from XML tags.\"\"\"\n",
        "\n",
        "    def __init__(self, fields=None):\n",
        "        self.fields = fields or []\n",
        "\n",
        "    def parse(self, text):\n",
        "        result = type('obj', (object,), {field: None for field in self.fields})\n",
        "\n",
        "        for field in self.fields:\n",
        "            pattern = f\"<{field}>(.*?)</{field}>\"\n",
        "            match = re.search(pattern, text, re.DOTALL)\n",
        "            if match:\n",
        "                setattr(result, field, match.group(1).strip())\n",
        "\n",
        "        return result\n",
        "\n",
        "def prepare_dataset_for_deepseek(demos_df, system_prompt=None, few_shot=None):\n",
        "    \"\"\"Prepare the dataset for DeepSeek training.\"\"\"\n",
        "    examples = []\n",
        "\n",
        "    for _, row in demos_df.iterrows():\n",
        "        try:\n",
        "            # Format the prompt\n",
        "            prompt_data = format_prompt(\n",
        "                row[\"state_str\"],\n",
        "                system_prompt=system_prompt,\n",
        "                few_shot=few_shot\n",
        "            )\n",
        "\n",
        "            example = {\n",
        "                \"prompt\": prompt_data,\n",
        "                \"action\": row[\"action\"],\n",
        "                \"action_text\": row[\"action_text\"],\n",
        "                \"state\": row[\"state\"],\n",
        "                \"q_values\": [\n",
        "                    row[\"q_value_left\"],\n",
        "                    row[\"q_value_down\"],\n",
        "                    row[\"q_value_right\"],\n",
        "                    row[\"q_value_up\"]\n",
        "                ],\n",
        "            }\n",
        "\n",
        "            examples.append(example)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing demo: {e}\")\n",
        "            continue\n",
        "\n",
        "    return pd.DataFrame(examples)\n",
        "\n",
        "def format_prompt(state_str, system_prompt=None, few_shot=None):\n",
        "    \"\"\"Format the prompt for the model.\"\"\"\n",
        "    messages = []\n",
        "\n",
        "    if system_prompt:\n",
        "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
        "\n",
        "    if few_shot:\n",
        "        messages.extend(few_shot)\n",
        "\n",
        "    user_content = f\"\"\"Current Frozen Lake state:\n",
        "{state_str}\n",
        "\n",
        "What action should I take to maximize my chances of reaching the goal safely?\n",
        "Respond with one of: LEFT, DOWN, RIGHT, UP.\n",
        "\n",
        "First think through your reasoning in <think> tags, then provide your final answer in <answer> tags.\n",
        "\"\"\"\n",
        "    messages.append({\"role\": \"user\", \"content\": user_content})\n",
        "\n",
        "    return messages\n",
        "\n",
        "class FrozenLakeEnv:\n",
        "    \"\"\"Environment for the Frozen Lake problem using verifiers framework.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        system_prompt=\"You are an expert at solving the Frozen Lake environment.\",\n",
        "        few_shot=None,\n",
        "        max_steps=1,\n",
        "    ):\n",
        "        self.system_prompt = system_prompt\n",
        "        self.few_shot = few_shot\n",
        "        self.max_steps = max_steps\n",
        "        self.parser = XMLParser(fields=[\"think\", \"answer\"])\n",
        "\n",
        "    def get_dataset(self):\n",
        "        \"\"\"Get the dataset for training.\"\"\"\n",
        "        # Load the expert demonstrations from CSV\n",
        "        demos_df = pd.read_csv(\"expert_demos_batched_avg_q.csv\")\n",
        "\n",
        "        # Convert pipe separators back to newlines in state_str\n",
        "        demos_df['state_str'] = demos_df['state_str'].str.replace('|', '\\n')\n",
        "\n",
        "        # Prepare dataset for DeepSeek\n",
        "        dataset = prepare_dataset_for_deepseek(\n",
        "            demos_df,\n",
        "            system_prompt=self.system_prompt,\n",
        "            few_shot=self.few_shot\n",
        "        )\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    def get_eval_dataset(self, n=100):\n",
        "        \"\"\"Get evaluation dataset.\"\"\"\n",
        "        full_dataset = self.get_dataset()\n",
        "\n",
        "        if len(full_dataset) > n:\n",
        "            return full_dataset.sample(n=n)\n",
        "        return full_dataset\n",
        "\n",
        "    def generate(self, prompts, llm, sampling_params):\n",
        "        \"\"\"Generate completions using the LLM.\"\"\"\n",
        "        completions = llm.generate(prompts, sampling_params)\n",
        "\n",
        "        ids = []\n",
        "        messages = []\n",
        "        masks = []\n",
        "\n",
        "        for completion in completions:\n",
        "            ids.append(completion.outputs[0].token_ids)\n",
        "            messages.append(completion.outputs[0].text)\n",
        "            mask = [1] * len(completion.outputs[0].token_ids)\n",
        "            masks.append(mask)\n",
        "\n",
        "        return {\n",
        "            'ids': ids,\n",
        "            'messages': messages,\n",
        "            'mask': masks\n",
        "        }\n",
        "\n",
        "    def get_rubric(self, **kwargs):\n",
        "        \"\"\"Get the reward functions for evaluation.\"\"\"\n",
        "\n",
        "        def correctness_reward_func(prompts, completions, **kwargs):\n",
        "            \"\"\"Reward function based on Q-values.\"\"\"\n",
        "            q_values_list = kwargs.get(\"q_values\", [])\n",
        "            expert_actions = kwargs.get(\"action\", [])\n",
        "\n",
        "            rewards = []\n",
        "            for completion, q_values, expert_action in zip(completions, q_values_list, expert_actions):\n",
        "                parsed = self.parser.parse(completion)\n",
        "                if parsed.answer is None:\n",
        "                    rewards.append(0.0)\n",
        "                    continue\n",
        "\n",
        "                action_map = {\"LEFT\": 0, \"DOWN\": 1, \"RIGHT\": 2, \"UP\": 3}\n",
        "                pred_action = action_map.get(parsed.answer.strip().upper(), -1)\n",
        "\n",
        "                if pred_action == -1:\n",
        "                    rewards.append(0.0)\n",
        "                    continue\n",
        "\n",
        "                expert_q_value = q_values[expert_action]\n",
        "                pred_q_value = q_values[pred_action]\n",
        "\n",
        "                q_max = max(q_values)\n",
        "                q_min = min(q_values)\n",
        "\n",
        "                if q_max > q_min:\n",
        "                    normalized_pred_q = (pred_q_value - q_min) / (q_max - q_min)\n",
        "                    exact_match_bonus = 0.2 if pred_action == expert_action else 0.0\n",
        "                    reward = normalized_pred_q + exact_match_bonus\n",
        "                    rewards.append(min(1.0, reward))\n",
        "                else:\n",
        "                    reward = 1.0 if pred_action == expert_action else 0.5\n",
        "                    rewards.append(reward)\n",
        "\n",
        "            return rewards\n",
        "\n",
        "        def format_reward_func(prompts, completions, **kwargs):\n",
        "            \"\"\"Reward function for proper XML formatting.\"\"\"\n",
        "            rewards = []\n",
        "\n",
        "            for completion in completions:\n",
        "                think_tag = re.search(r\"<think>(.*?)</think>\", completion, re.DOTALL)\n",
        "                answer_tag = re.search(r\"<answer>(.*?)</answer>\", completion, re.DOTALL)\n",
        "\n",
        "                if think_tag and answer_tag:\n",
        "                    rewards.append(1.0)\n",
        "                elif answer_tag:\n",
        "                    rewards.append(0.5)\n",
        "                else:\n",
        "                    rewards.append(0.0)\n",
        "\n",
        "            return rewards\n",
        "\n",
        "        return [correctness_reward_func, format_reward_func]\n",
        "\n",
        "def train_deepseek_on_frozen_lake():\n",
        "    \"\"\"Train DeepSeek on Frozen Lake using Q-learning demonstrations.\"\"\"\n",
        "    try:\n",
        "        import torch\n",
        "        import verifiers as vf\n",
        "        from verifiers.trainers.grpo_env_trainer import GRPOEnvTrainer\n",
        "    except ImportError as e:\n",
        "        print(f\"Error importing torch or verifiers: {e}\")\n",
        "        print(\"This functionality requires PyTorch and the verifiers package.\")\n",
        "        return\n",
        "\n",
        "    # Initialize environment\n",
        "    fl_env = FrozenLakeEnv()\n",
        "\n",
        "    # Get dataset and rubric\n",
        "    dataset = fl_env.get_dataset()\n",
        "    eval_dataset = fl_env.get_eval_dataset(n=100)\n",
        "    rubric = fl_env.get_rubric()\n",
        "\n",
        "    # Load model and tokenizer\n",
        "    model, tokenizer = vf.get_model_and_tokenizer(\"deepseek-ai/deepseek-llm-7b-base\")\n",
        "\n",
        "    # Training arguments\n",
        "    run_name = \"frozen-lake-deepseek-qlearning\"\n",
        "    training_args = vf.get_default_grpo_config(\n",
        "        run_name=run_name,\n",
        "        num_gpus=1\n",
        "    )\n",
        "\n",
        "    # Configure training parameters\n",
        "    training_args.num_generations = 7  # Rollouts per prompt\n",
        "    training_args.per_device_train_batch_size = 6  # Minibatch size per GPU\n",
        "    training_args.gradient_accumulation_steps = 4  # Batches to accumulate\n",
        "    training_args.num_iterations = 2  # Steps per global batch\n",
        "    training_args.beta = 0.04  # KL penalty\n",
        "    training_args.use_vllm = True  # Enable vLLM\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = GRPOEnvTrainer(\n",
        "        model=model,\n",
        "        processing_class=tokenizer,\n",
        "        reward_funcs=rubric,\n",
        "        env=fl_env,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "    )\n",
        "\n",
        "    # Start training\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the final model\n",
        "    trainer.save_model(\"frozen-lake-deepseek-final\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_deepseek_on_frozen_lake()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c66164ed-68e2-4ecb-a1fe-f9c6e419faa2",
      "metadata": {
        "scrolled": true,
        "id": "c66164ed-68e2-4ecb-a1fe-f9c6e419faa2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Dict, List, Any, Optional, Union, Tuple\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainerCallback\n",
        "from trl import GRPOTrainer, GRPOConfig\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Set PyTorch memory allocation configuration\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# Set Hugging Face cache directory to /scratch/athanina\n",
        "os.environ[\"HF_HOME\"] = \"/scratch/athanina/huggingface_cache\"\n",
        "os.makedirs(os.environ[\"HF_HOME\"], exist_ok=True)\n",
        "\n",
        "# --- XML Parser ---\n",
        "class XMLParser:\n",
        "    \"\"\"Parser for extracting content from XML tags.\"\"\"\n",
        "    def __init__(self, fields=None):\n",
        "        self.fields = fields or []\n",
        "\n",
        "    def parse(self, text):\n",
        "        \"\"\"Parse text to extract content within specified XML tags.\"\"\"\n",
        "        result = type('obj', (object,), {field: None for field in self.fields})\n",
        "        for field in self.fields:\n",
        "            pattern = f\"<{field}>(.*?)</{field}>\"\n",
        "            match = re.search(pattern, text, re.DOTALL)\n",
        "            if match:\n",
        "                setattr(result, field, match.group(1).strip())\n",
        "        return result\n",
        "\n",
        "# --- Prompt Formatting ---\n",
        "def format_prompt(state_str, system_prompt=None, few_shot=None):\n",
        "    \"\"\"Format the prompt for the Frozen Lake task with few-shot examples.\"\"\"\n",
        "    messages = []\n",
        "    if system_prompt:\n",
        "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
        "    if few_shot:\n",
        "        messages.extend(few_shot)\n",
        "\n",
        "    user_content = f\"\"\"Current Frozen Lake state:\n",
        "{state_str}\n",
        "\n",
        "What action should I take to maximize my chances of reaching the goal safely?\n",
        "Respond with one of: LEFT, DOWN, RIGHT, UP.\n",
        "\n",
        "First think through your reasoning in <think> tags, then provide your final answer in <answer> tags.\n",
        "\"\"\"\n",
        "    messages.append({\"role\": \"user\", \"content\": user_content})\n",
        "    return \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in messages])\n",
        "\n",
        "# --- Frozen Lake Environment ---\n",
        "class FrozenLakeEnv:\n",
        "    def __init__(\n",
        "        self,\n",
        "        system_prompt=\"You are an expert at solving the Frozen Lake environment.\",\n",
        "        max_steps=1,\n",
        "    ):\n",
        "        \"\"\"Initialize the Frozen Lake environment for RL training.\"\"\"\n",
        "        self.system_prompt = system_prompt\n",
        "        self.max_steps = max_steps\n",
        "        self.parser = XMLParser(fields=[\"think\", \"answer\"])\n",
        "        self.few_shot = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"\"\"Current Frozen Lake state:\n",
        "S F F F\n",
        "F H F H\n",
        "F F F H\n",
        "H F F G\n",
        "\n",
        "What action should I take to maximize my chances of reaching the goal safely?\n",
        "Respond with one of: LEFT, DOWN, RIGHT, UP.\"\"\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"\"\"<think>The goal (G) is at the bottom-right (3,3). From the start (S) at (0,0), moving RIGHT to (0,1) 'F' keeps me on safe ice and progresses toward the goal.</think>\n",
        "<answer>RIGHT</answer>\"\"\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"\"\"Current Frozen Lake state:\n",
        "F F S F\n",
        "F H F H\n",
        "F F F H\n",
        "H F F G\n",
        "\n",
        "What action should I take to maximize my chances of reaching the goal safely?\n",
        "Respond with one of: LEFT, DOWN, RIGHT, UP.\"\"\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"\"\"<think>I'm at (0,2), and the goal is at (3,3). Moving DOWN to (1,2) 'F' is safe and brings me closer to the goal vertically.</think>\n",
        "<answer>DOWN</answer>\"\"\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"\"\"Current Frozen Lake state:\n",
        "F S F F\n",
        "F H F H\n",
        "F F F H\n",
        "H F F G\n",
        "\n",
        "What action should I take to maximize my chances of reaching the goal safely?\n",
        "Respond with one of: LEFT, DOWN, RIGHT, UP.\"\"\"\n",
        "            }\n",
        "\n",
        "\n",
        "        ]\n",
        "        self.dataset = self._load_dataset()\n",
        "\n",
        "    def _load_dataset(self):\n",
        "        \"\"\"Load and validate expert demonstrations from CSV.\"\"\"\n",
        "        try:\n",
        "            demos_df = pd.read_csv(\"expert_demos_batched_avg_q.csv\")\n",
        "            required_cols = [\"state_str\", \"action\", \"q_value_left\", \"q_value_down\", \"q_value_right\", \"q_value_up\"]\n",
        "            if not all(col in demos_df.columns for col in required_cols):\n",
        "                raise ValueError(f\"CSV missing required columns: {set(required_cols) - set(demos_df.columns)}\")\n",
        "            demos_df = demos_df.dropna(subset=required_cols[1:])\n",
        "            demos_df['state_str'] = demos_df['state_str'].str.replace('|', '\\n')\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to load dataset: {e}\")\n",
        "\n",
        "        examples = []\n",
        "        for _, row in demos_df.iterrows():\n",
        "            prompt = format_prompt(\n",
        "                row[\"state_str\"],\n",
        "                system_prompt=self.system_prompt,\n",
        "                few_shot=self.few_shot\n",
        "            )\n",
        "            examples.append({\n",
        "                \"prompt\": prompt,\n",
        "                \"action\": row[\"action\"],\n",
        "                \"q_values\": [\n",
        "                    row[\"q_value_left\"],\n",
        "                    row[\"q_value_down\"],\n",
        "                    row[\"q_value_right\"],\n",
        "                    row[\"q_value_up\"]\n",
        "                ]\n",
        "            })\n",
        "        return pd.DataFrame(examples)\n",
        "\n",
        "    def get_dataset(self, n=None):\n",
        "        \"\"\"Return a subset or full dataset.\"\"\"\n",
        "        if n and len(self.dataset) > n:\n",
        "            return self.dataset.sample(n=n)\n",
        "        return self.dataset\n",
        "\n",
        "    def evaluate_action(self, completion, q_values, expert_action):\n",
        "        \"\"\"Calculate reward based on Q-values and formatting.\"\"\"\n",
        "        parsed = self.parser.parse(completion)\n",
        "        think_tag = bool(re.search(r\"<think>(.*?)</think>\", completion, re.DOTALL))\n",
        "        answer_tag = bool(re.search(r\"<answer>(.*?)</answer>\", completion, re.DOTALL))\n",
        "        format_reward = 1.0 if (think_tag and answer_tag) else 0.0\n",
        "\n",
        "        if parsed.answer is None:\n",
        "            correctness_reward = 0.0\n",
        "        else:\n",
        "            action_map = {\"LEFT\": 0, \"DOWN\": 1, \"RIGHT\": 2, \"UP\": 3}\n",
        "            pred_action = action_map.get(parsed.answer.strip().upper(), -1)\n",
        "            if pred_action == -1:\n",
        "                correctness_reward = 0.0\n",
        "            else:\n",
        "                expert_q_value = q_values[expert_action]\n",
        "                pred_q_value = q_values[pred_action]\n",
        "                q_max, q_min = max(q_values), min(q_values)\n",
        "                if q_max > q_min:\n",
        "                    normalized_pred_q = (pred_q_value - q_min) / (q_max - q_min)\n",
        "                    exact_match_bonus = 0.2 if pred_action == expert_action else 0.0\n",
        "                    correctness_reward = min(1.0, normalized_pred_q + exact_match_bonus)\n",
        "                else:\n",
        "                    correctness_reward = 1.0 if pred_action == expert_action else 0.5\n",
        "\n",
        "        return (format_reward + correctness_reward) / 2\n",
        "\n",
        "    def get_rubric(self):\n",
        "        \"\"\"Return the primary reward function for GRPO training.\"\"\"\n",
        "        def combined_reward_func(prompts, completions, **kwargs):\n",
        "            q_values_list = kwargs[\"q_values\"]\n",
        "            expert_actions = kwargs[\"action\"]\n",
        "            rewards = []\n",
        "            for i, (completion, q_values, expert_action) in enumerate(zip(completions, q_values_list, expert_actions)):\n",
        "                # Use the existing evaluate_action which combines format and correctness\n",
        "                reward = self.evaluate_action(completion, q_values, expert_action)\n",
        "                if reward == 0.0 and self.parser.parse(completion).answer is None:\n",
        "                     # Keep the warning for debugging\n",
        "                    print(f\"Warning: Low reward (likely format/parse issue) in completion {i}: {completion}\")\n",
        "                rewards.append(reward)\n",
        "            return rewards\n",
        "\n",
        "        # Return only the single combined reward function\n",
        "        return [combined_reward_func]\n",
        "\n",
        "# --- Custom Logging Callback ---\n",
        "class LoggingCallback(TrainerCallback):\n",
        "    \"\"\"Callback to log training progress for research analysis.\"\"\"\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if logs:\n",
        "            print(f\"Step {state.global_step}: {logs}\")\n",
        "\n",
        "    def on_step_end(self, args, state, control, **kwargs):\n",
        "        if state.global_step % 10 == 0:\n",
        "            print(f\"Step {state.global_step} - Sample completion: {kwargs.get('outputs', 'No outputs yet')}\")\n",
        "\n",
        "# --- Training Function ---\n",
        "def train_deepseek_on_frozen_lake_with_grpo():\n",
        "    \"\"\"Train DeepSeek on Frozen Lake using GRPO with expert Q-value demonstrations on an A100.\"\"\"\n",
        "    env = FrozenLakeEnv()\n",
        "    # Reduce dataset size to manage memory usage\n",
        "    train_dataset = env.get_dataset(n=500)\n",
        "    eval_dataset = env.get_dataset(n=100)\n",
        "    reward_funcs = env.get_rubric()\n",
        "\n",
        "    # Load DeepSeek model and tokenizer\n",
        "    model_name = \"google/gemma-2b-it\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_name,\n",
        "        padding_side=\"left\",\n",
        "        truncation=True,\n",
        "        max_length=1024\n",
        "    )\n",
        "\n",
        "    # Load model with automatic device map for memory optimization\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16,  # Changed from bfloat16 to float16\n",
        "        device_map=\"auto\"  # Use auto device mapping for memory optimization\n",
        "    )\n",
        "\n",
        "    # Configure LoRA for parameter-efficient fine-tuning\n",
        "    lora_config = LoraConfig(\n",
        "        r=16,  # Reduced from typical 16 to save memory\n",
        "        lora_alpha=16,\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\"\n",
        "    )\n",
        "    model = get_peft_model(model, lora_config)\n",
        "\n",
        "    # Verify which parameters will be trained\n",
        "    print(\"Checking trainable parameters:\")\n",
        "    trainable_params = 0\n",
        "    all_params = 0\n",
        "    for name, param in model.named_parameters():\n",
        "        all_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "            print(f\"{name}: requires_grad=True, shape={param.shape}, dtype={param.dtype}\")\n",
        "\n",
        "    print(f\"Trainable params: {trainable_params} ({100 * trainable_params / all_params:.2f}% of all params)\")\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    train_dataset = Dataset.from_pandas(train_dataset)\n",
        "    eval_dataset = Dataset.from_pandas(eval_dataset)\n",
        "\n",
        "    # GRPO configuration with memory optimizations\n",
        "    grpo_config = GRPOConfig(\n",
        "        output_dir=\"./gemma-2b-it\",\n",
        "        num_train_epochs=10,\n",
        "        per_device_train_batch_size=2,  # Reduced from 8\n",
        "        per_device_eval_batch_size=2,   # Reduced from 8\n",
        "        gradient_accumulation_steps=8,  # Increased from 2\n",
        "        learning_rate=3e-5,\n",
        "        weight_decay=0.01,\n",
        "        logging_steps=5,\n",
        "        save_steps=100,  # Increased to reduce disk I/O\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=50,   # Increased to reduce evaluation frequency\n",
        "        fp16=True,       # Use FP16 precision\n",
        "        bf16=False,      # Ensure BF16 is disabled\n",
        "        report_to=\"none\",\n",
        "        num_generations=4,  # Reduced from 4\n",
        "        beta=0.1,\n",
        "        gradient_checkpointing=True,\n",
        "        optim=\"adamw_torch\",  # Specify optimizer explicitly\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        warmup_ratio=0.1,\n",
        "        max_grad_norm=1.0,\n",
        "        # Additional memory optimizations\n",
        "        deepspeed=None,  # Disable DeepSpeed if not needed\n",
        "        ddp_find_unused_parameters=False,\n",
        "    )\n",
        "\n",
        "    # Initialize GRPO trainer\n",
        "    trainer = GRPOTrainer(\n",
        "        model=model,\n",
        "        args=grpo_config,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        processing_class=tokenizer,\n",
        "        reward_funcs=reward_funcs,\n",
        "        callbacks=[LoggingCallback()]\n",
        "    )\n",
        "\n",
        "    print(\"Starting training on NVIDIA A100...\")\n",
        "    try:\n",
        "        trainer.train()\n",
        "        success = True\n",
        "    except Exception as e:\n",
        "        print(f\"Training error: {e}\")\n",
        "        success = False\n",
        "\n",
        "    if success:\n",
        "        trainer.save_model(\"./frozen-lake-gemma-2b-it-final\")\n",
        "        tokenizer.save_pretrained(\"./frozen-lake-gemma-2b-it-final\")\n",
        "        print(\"Training completed. Model saved to './frozen-lake-gemma-2b-it-final'.\")\n",
        "    else:\n",
        "        # Try to save checkpoint even if training fails\n",
        "        try:\n",
        "            trainer.save_model(\"./frozen-lake-gemma-2b-it-checkpoint\")\n",
        "            tokenizer.save_pretrained(\"./frozen-lake-gemma-2b-it-checkpoint\")\n",
        "            print(\"Training failed but checkpoint saved to './frozen-lake-gemma-2b-it-checkpoint'.\")\n",
        "        except:\n",
        "            print(\"Could not save checkpoint.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_deepseek_on_frozen_lake_with_grpo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91b4dbba-29f4-4856-b48e-f30a70b5a771",
      "metadata": {
        "id": "91b4dbba-29f4-4856-b48e-f30a70b5a771"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Dict, List, Any, Optional, Union, Tuple\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainerCallback\n",
        "from trl import GRPOTrainer, GRPOConfig\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Set PyTorch memory allocation configuration\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# Set Hugging Face cache directory to /scratch/athanina\n",
        "os.environ[\"HF_HOME\"] = \"/scratch/athanina/huggingface_cache\"\n",
        "os.makedirs(os.environ[\"HF_HOME\"], exist_ok=True)\n",
        "\n",
        "# --- XML Parser ---\n",
        "class XMLParser:\n",
        "    \"\"\"Parser for extracting content from XML tags.\"\"\"\n",
        "    def __init__(self, fields=None):\n",
        "        self.fields = fields or []\n",
        "\n",
        "    def parse(self, text):\n",
        "        \"\"\"Parse text to extract content within specified XML tags.\"\"\"\n",
        "        result = type('obj', (object,), {field: None for field in self.fields})() # Create instance\n",
        "        for field in self.fields:\n",
        "            pattern = f\"<{field}>(.*?)</{field}>\"\n",
        "            match = re.search(pattern, text, re.DOTALL)\n",
        "            if match:\n",
        "                setattr(result, field, match.group(1).strip())\n",
        "        return result\n",
        "\n",
        "# --- Prompt Formatting ---\n",
        "def format_prompt(state_str, system_prompt=None, few_shot=None):\n",
        "    \"\"\"Format the prompt for the Frozen Lake task with few-shot examples.\"\"\"\n",
        "    messages = []\n",
        "    if system_prompt:\n",
        "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
        "    if few_shot:\n",
        "        messages.extend(few_shot)\n",
        "\n",
        "    user_content = f\"\"\"Current Frozen Lake state:\n",
        "{state_str}\n",
        "\n",
        "What action should I take to maximize my chances of reaching the goal safely?\n",
        "Respond with one of: LEFT, DOWN, RIGHT, UP.\n",
        "\n",
        "First think through your reasoning in <think> tags, then provide your final answer in <answer> tags.\n",
        "\"\"\"\n",
        "    messages.append({\"role\": \"user\", \"content\": user_content})\n",
        "    # Gemma instruction format uses specific tokens\n",
        "    # Format: \"<start_of_turn>user\\n{user_content}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "    prompt_str = \"<start_of_turn>user\\n\"\n",
        "    if system_prompt:\n",
        "         prompt_str += system_prompt + \"\\n\\n\" # Add system prompt if present\n",
        "    if few_shot:\n",
        "        for i in range(0, len(few_shot), 2):\n",
        "            prompt_str += few_shot[i]['content'] # User part\n",
        "            prompt_str += \"<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "            prompt_str += few_shot[i+1]['content'] # Assistant part\n",
        "            prompt_str += \"<end_of_turn>\\n<start_of_turn>user\\n\" # Prepare for next user turn\n",
        "\n",
        "    prompt_str += user_content # Add the final user query\n",
        "    prompt_str += \"<end_of_turn>\\n<start_of_turn>model\\n\" # Signal model to respond\n",
        "\n",
        "    # Note: The above formatting assumes the tokenizer adds BOS/EOS appropriately.\n",
        "    # Depending on the specific Gemma tokenizer usage, slight adjustments might be needed.\n",
        "    # Let's simplify for now and assume the tokenizer handles the structure.\n",
        "    # Reverting to simpler format for clarity, check Gemma docs if issues arise.\n",
        "    messages_formatted = []\n",
        "    if system_prompt:\n",
        "         messages_formatted.append(f\"System: {system_prompt}\")\n",
        "    if few_shot:\n",
        "        for msg in few_shot:\n",
        "            messages_formatted.append(f\"{msg['role']}: {msg['content']}\")\n",
        "    messages_formatted.append(f\"user: {user_content}\")\n",
        "    messages_formatted.append(\"model:\") # Prompt model to start generating\n",
        "\n",
        "    # Using a simple join for now, ensure tokenizer handles roles if needed\n",
        "    return \"\\n\".join(messages_formatted)\n",
        "\n",
        "\n",
        "# --- Frozen Lake Environment ---\n",
        "class FrozenLakeEnv:\n",
        "    def __init__(\n",
        "        self,\n",
        "        system_prompt=\"You are an expert at solving the Frozen Lake environment. Analyze the state and provide your reasoning before giving the action.\",\n",
        "        max_steps=1, # Unused currently\n",
        "    ):\n",
        "        \"\"\"Initialize the Frozen Lake environment for RL training.\"\"\"\n",
        "        self.system_prompt = system_prompt\n",
        "        self.max_steps = max_steps\n",
        "        self.parser = XMLParser(fields=[\"think\", \"answer\"])\n",
        "        # Reduced few-shot examples\n",
        "        self.few_shot = [\n",
        "             {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"\"\"Current Frozen Lake state:\n",
        "S F F F\n",
        "F H F H\n",
        "F F F H\n",
        "H F F G\n",
        "\n",
        "What action should I take to maximize my chances of reaching the goal safely?\n",
        "Respond with one of: LEFT, DOWN, RIGHT, UP.\n",
        "\n",
        "First think through your reasoning in <think> tags, then provide your final answer in <answer> tags.\"\"\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"\"\"<think>The goal (G) is at the bottom-right (3,3). From the start (S) at (0,0), moving RIGHT to (0,1) 'F' keeps me on safe ice and progresses toward the goal.</think>\n",
        "<answer>RIGHT</answer>\"\"\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"\"\"Current Frozen Lake state:\n",
        "F F S F\n",
        "F H F H\n",
        "F F F H\n",
        "H F F G\n",
        "\n",
        "What action should I take to maximize my chances of reaching the goal safely?\n",
        "Respond with one of: LEFT, DOWN, RIGHT, UP.\n",
        "\n",
        "First think through your reasoning in <think> tags, then provide your final answer in <answer> tags.\"\"\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"\"\"<think>I'm at (0,2), and the goal is at (3,3). Moving DOWN to (1,2) 'F' is safe and brings me closer to the goal vertically.</think>\n",
        "<answer>DOWN</answer>\"\"\"\n",
        "            },\n",
        "             { # Adding one more distinct example\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"\"\"Current Frozen Lake state:\n",
        "F F F F\n",
        "F H F H\n",
        "F F S H\n",
        "H F F G\n",
        "\n",
        "What action should I take to maximize my chances of reaching the goal safely?\n",
        "Respond with one of: LEFT, DOWN, RIGHT, UP.\n",
        "\n",
        "First think through your reasoning in <think> tags, then provide your final answer in <answer> tags.\"\"\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"\"\"<think>I'm at (2,2), and the goal is at (3,3). Moving DOWN to (3,2) 'F' is safe, then I can move RIGHT to (3,3) 'G' to reach the goal.</think>\n",
        "<answer>DOWN</answer>\"\"\"\n",
        "            }\n",
        "        ]\n",
        "        self.dataset = self._load_dataset()\n",
        "        self.action_map = {\"LEFT\": 0, \"DOWN\": 1, \"RIGHT\": 2, \"UP\": 3}\n",
        "        self.reverse_action_map = {v: k for k, v in self.action_map.items()}\n",
        "\n",
        "\n",
        "    def _load_dataset(self):\n",
        "        \"\"\"Load and validate expert demonstrations from CSV.\"\"\"\n",
        "        try:\n",
        "            demos_df = pd.read_csv(\"expert_demos_batched_avg_q.csv\")\n",
        "            required_cols = [\"state_str\", \"action\", \"q_value_left\", \"q_value_down\", \"q_value_right\", \"q_value_up\"]\n",
        "            if not all(col in demos_df.columns for col in required_cols):\n",
        "                raise ValueError(f\"CSV missing required columns: {set(required_cols) - set(demos_df.columns)}\")\n",
        "            demos_df = demos_df.dropna(subset=required_cols[1:]) # Drop rows with missing Q-values or action\n",
        "             # Ensure action is integer if it's not already\n",
        "            if demos_df['action'].dtype != np.int64 and demos_df['action'].dtype != np.int32:\n",
        "                 # Assuming action might be stored as string name, map it\n",
        "                 action_map_load = {\"LEFT\": 0, \"DOWN\": 1, \"RIGHT\": 2, \"UP\": 3}\n",
        "                 # Handle potential errors during mapping\n",
        "                 demos_df['action'] = demos_df['action'].apply(lambda x: action_map_load.get(str(x).upper(), -1))\n",
        "                 demos_df = demos_df[demos_df['action'] != -1] # Remove rows with invalid actions\n",
        "            demos_df['state_str'] = demos_df['state_str'].str.replace('|', '\\n', regex=False)\n",
        "\n",
        "        except FileNotFoundError:\n",
        "             raise RuntimeError(\"Failed to load dataset: expert_demos_batched_avg_q.csv not found.\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to load or process dataset: {e}\")\n",
        "\n",
        "        examples = []\n",
        "        for _, row in demos_df.iterrows():\n",
        "            prompt = format_prompt(\n",
        "                row[\"state_str\"],\n",
        "                system_prompt=self.system_prompt,\n",
        "                few_shot=self.few_shot\n",
        "            )\n",
        "            # Ensure Q-values are floats\n",
        "            q_values = [\n",
        "                float(row[\"q_value_left\"]),\n",
        "                float(row[\"q_value_down\"]),\n",
        "                float(row[\"q_value_right\"]),\n",
        "                float(row[\"q_value_up\"])\n",
        "            ]\n",
        "            examples.append({\n",
        "                \"prompt\": prompt,\n",
        "                \"action\": int(row[\"action\"]), # Ensure action is integer index\n",
        "                \"q_values\": q_values\n",
        "            })\n",
        "        return pd.DataFrame(examples)\n",
        "\n",
        "    def get_dataset(self, n=None):\n",
        "        \"\"\"Return a subset or full dataset.\"\"\"\n",
        "        if n and len(self.dataset) > n:\n",
        "            # Use random_state for reproducibility if needed\n",
        "            return self.dataset.sample(n=n, random_state=42)\n",
        "        return self.dataset\n",
        "\n",
        "    def evaluate_action(self, completion, q_values, expert_action_index):\n",
        "        \"\"\"Calculate reward based on Q-values and formatting.\"\"\"\n",
        "        parsed = self.parser.parse(completion)\n",
        "        think_tag = bool(re.search(r\"<think>(.*?)</think>\", completion, re.DOTALL))\n",
        "        answer_tag = bool(re.search(r\"<answer>(.*?)</answer>\", completion, re.DOTALL))\n",
        "        format_reward = 1.0 if (think_tag and answer_tag) else 0.0\n",
        "\n",
        "        correctness_reward = 0.0 # Default to 0\n",
        "        if parsed.answer is not None:\n",
        "            pred_action_str = parsed.answer.strip().upper()\n",
        "            pred_action_index = self.action_map.get(pred_action_str, -1)\n",
        "\n",
        "            if pred_action_index != -1:\n",
        "                # Ensure Q-values are valid numbers\n",
        "                if not all(isinstance(q, (int, float)) for q in q_values):\n",
        "                     print(f\"Warning: Invalid Q-values encountered: {q_values}\")\n",
        "                     # Handle invalid Q-values, e.g., return minimal reward or skip\n",
        "                     return format_reward * 0.1 # Penalize heavily but keep format signal\n",
        "\n",
        "                expert_q_value = q_values[expert_action_index]\n",
        "                pred_q_value = q_values[pred_action_index]\n",
        "                q_max, q_min = max(q_values), min(q_values)\n",
        "\n",
        "                if q_max > q_min:\n",
        "                    # Normalize predicted Q-value\n",
        "                    normalized_pred_q = (pred_q_value - q_min) / (q_max - q_min)\n",
        "                    # Add bonus for exact match with expert action index\n",
        "                    exact_match_bonus = 0.2 if pred_action_index == expert_action_index else 0.0\n",
        "                    # Combine normalized Q + bonus, capped at 1.0\n",
        "                    correctness_reward = min(1.0, normalized_pred_q + exact_match_bonus)\n",
        "                else:\n",
        "                    # Handle case where all Q-values are the same\n",
        "                    correctness_reward = 1.0 if pred_action_index == expert_action_index else 0.5\n",
        "            # else: pred_action_index == -1 (invalid action string), correctness_reward remains 0.0\n",
        "\n",
        "        # Combine format and correctness rewards (equal weighting)\n",
        "        final_reward = (format_reward + correctness_reward) / 2.0\n",
        "        return final_reward\n",
        "\n",
        "\n",
        "    # Updated rubric as suggested\n",
        "    def get_rubric(self):\n",
        "        \"\"\"Return the primary reward function for GRPO training.\"\"\"\n",
        "        def combined_reward_func(prompts: List[str], completions: List[str], **kwargs) -> List[float]:\n",
        "            # Ensure necessary kwargs are present\n",
        "            if \"q_values\" not in kwargs or \"action\" not in kwargs:\n",
        "                 raise ValueError(\"Missing 'q_values' or 'action' in reward function kwargs\")\n",
        "\n",
        "            q_values_list = kwargs[\"q_values\"]\n",
        "            expert_actions_indices = kwargs[\"action\"] # Assuming 'action' is the expert index\n",
        "            rewards = []\n",
        "\n",
        "            if not (len(completions) == len(q_values_list) == len(expert_actions_indices)):\n",
        "                 print(f\"Warning: Mismatch in lengths - completions: {len(completions)}, q_values: {len(q_values_list)}, actions: {len(expert_actions_indices)}\")\n",
        "                 # Handle mismatch, e.g., return default rewards or skip batch\n",
        "                 return [0.0] * len(completions) # Example: return 0 reward for all\n",
        "\n",
        "            for i, (completion, q_values, expert_action_index) in enumerate(zip(completions, q_values_list, expert_actions_indices)):\n",
        "                # Ensure expert_action_index is valid\n",
        "                if not isinstance(expert_action_index, int) or not (0 <= expert_action_index < 4):\n",
        "                     print(f\"Warning: Invalid expert action index {expert_action_index} at index {i}. Skipping reward calculation.\")\n",
        "                     rewards.append(0.0) # Assign zero reward for invalid data\n",
        "                     continue\n",
        "\n",
        "                # Ensure q_values is a list of numbers\n",
        "                if not isinstance(q_values, list) or len(q_values) != 4:\n",
        "                     print(f\"Warning: Invalid q_values format {q_values} at index {i}. Skipping reward calculation.\")\n",
        "                     rewards.append(0.0)\n",
        "                     continue\n",
        "\n",
        "                reward = self.evaluate_action(completion, q_values, expert_action_index)\n",
        "                # Optional: Add more verbose logging for debugging low rewards\n",
        "                # if reward < 0.1:\n",
        "                #    print(f\"Debug: Low reward ({reward:.2f}) for completion {i}: {completion[:100]}... | Expert Action Index: {expert_action_index} | Q-values: {q_values}\")\n",
        "                rewards.append(reward)\n",
        "            return rewards\n",
        "\n",
        "        # Return only the single combined reward function in a list\n",
        "        return [combined_reward_func]\n",
        "\n",
        "\n",
        "# --- Custom Logging Callback ---\n",
        "class LoggingCallback(TrainerCallback):\n",
        "    \"\"\"Callback to log training progress and sample completions.\"\"\"\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if logs:\n",
        "            # Basic console logging of metrics reported by Trainer\n",
        "            print(f\"Step {state.global_step}: {logs}\")\n",
        "\n",
        "    # Optional: Log sample completions periodically (can be verbose)\n",
        "    # def on_step_end(self, args, state, control, model=None, tokenizer=None, **kwargs):\n",
        "    #     if state.global_step % 50 == 0 and state.global_step > 0: # Log every 50 steps\n",
        "    #         if 'eval_dataloader' in kwargs: # Check if eval dataloader is available\n",
        "    #             try:\n",
        "    #                 # Get a sample from the eval dataset\n",
        "    #                 eval_loader = kwargs['eval_dataloader']\n",
        "    #                 batch = next(iter(eval_loader))\n",
        "    #                 prompt_text = tokenizer.decode(batch['prompt_input_ids'][0], skip_special_tokens=True)\n",
        "    #                 # Generate a completion\n",
        "    #                 inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n",
        "    #                 # Adjust generation parameters as needed\n",
        "    #                 outputs = model.generate(**inputs, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)\n",
        "    #                 completion_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    #                 print(f\"\\n--- Sample Completion at Step {state.global_step} ---\")\n",
        "    #                 print(f\"Prompt:\\n{prompt_text}\")\n",
        "    #                 print(f\"Completion:\\n{completion_text}\\n--------------------------------\\n\")\n",
        "    #             except Exception as e:\n",
        "    #                 print(f\"Error generating sample completion at step {state.global_step}: {e}\")\n",
        "\n",
        "\n",
        "# --- Training Function ---\n",
        "def train_gemma_on_frozen_lake_with_grpo():\n",
        "    \"\"\"Train Gemma on Frozen Lake using GRPO with expert Q-value demonstrations.\"\"\"\n",
        "    print(\"Initializing environment and loading dataset...\")\n",
        "    env = FrozenLakeEnv()\n",
        "    # Use a reasonable subset for training and validation\n",
        "    train_dataset = env.get_dataset(n=1000) # Increased dataset size slightly\n",
        "    eval_dataset = env.get_dataset(n=200)\n",
        "    reward_funcs = env.get_rubric()\n",
        "\n",
        "    print(f\"Loaded {len(train_dataset)} training examples and {len(eval_dataset)} evaluation examples.\")\n",
        "\n",
        "    # Load Gemma model and tokenizer\n",
        "    model_name = \"google/gemma-2b-it\" # Changed to Gemma 2B Instruct\n",
        "    print(f\"Loading model and tokenizer: {model_name}...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_name,\n",
        "        padding_side=\"left\", # Important for generation\n",
        "        truncation=True,\n",
        "        max_length=768 # Increased max_length slightly from 512\n",
        "    )\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.bfloat16, # Use bfloat16 on A100\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    print(\"Model and tokenizer loaded.\")\n",
        "\n",
        "    # Configure LoRA\n",
        "    lora_config = LoraConfig(\n",
        "        r=16, # Increased rank slightly as Gemma 2B is smaller\n",
        "        lora_alpha=32, # Standard practice: alpha = 2*r\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], # Target attention layers\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\"\n",
        "    )\n",
        "    print(\"Applying LoRA configuration...\")\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    model.print_trainable_parameters() # Verify trainable parameters\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        print(\"Tokenizer does not have a pad token, setting it to eos_token.\")\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        # Important: Also update model config if needed, though PEFT/Trainer usually handle this\n",
        "        model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "\n",
        "    print(\"Converting pandas DataFrames to Hugging Face Datasets...\")\n",
        "    train_dataset_hf = Dataset.from_pandas(train_dataset)\n",
        "    eval_dataset_hf = Dataset.from_pandas(eval_dataset)\n",
        "    print(\"Dataset conversion complete.\")\n",
        "\n",
        "    # GRPO configuration\n",
        "    output_dir = \"./frozen-lake-gemma-2b-it-grpo\"\n",
        "    grpo_config = GRPOConfig(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=5, # Reduced epochs slightly, instruct model might learn faster\n",
        "        per_device_train_batch_size=4,  # Increased batch size due to smaller model\n",
        "        per_device_eval_batch_size=4,   # Increased batch size\n",
        "        gradient_accumulation_steps=4,  # Adjusted grad accum (effective batch size 16)\n",
        "        learning_rate=3e-5, # Slightly increased LR for smaller model/LoRA\n",
        "        weight_decay=0.01,\n",
        "        logging_steps=10, # Log more frequently\n",
        "        save_steps=100,\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=50,\n",
        "        bf16=True,       # Use BF16 precision on A100\n",
        "        fp16=False,      # Disable FP16\n",
        "        report_to=\"tensorboard\", # <--- Enable TensorBoard logging\n",
        "        num_generations=4,  # Increased generations back\n",
        "        beta=0.1, # Adjusted beta slightly\n",
        "        gradient_checkpointing=True,\n",
        "        optim=\"adamw_torch\",\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        warmup_ratio=0.1,\n",
        "        max_grad_norm=1.0,\n",
        "        remove_unused_columns=False, # Important: Keep 'q_values' and 'action' for reward func\n",
        "        ddp_find_unused_parameters=False,\n",
        "    )\n",
        "\n",
        "    # Initialize GRPO trainer\n",
        "    print(\"Initializing GRPOTrainer...\")\n",
        "    trainer = GRPOTrainer(\n",
        "        model=model,\n",
        "        args=grpo_config,\n",
        "        train_dataset=train_dataset_hf,\n",
        "        eval_dataset=eval_dataset_hf,\n",
        "        tokenizer=tokenizer, # Pass tokenizer directly\n",
        "        reward_funcs=reward_funcs,\n",
        "        callbacks=[LoggingCallback()] # Keep console logging\n",
        "    )\n",
        "\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\"      Starting GRPO Training with Gemma 2B      \")\n",
        "    print(\"=\"*40 + \"\\n\")\n",
        "    print(\"To monitor training with TensorBoard:\")\n",
        "    print(f\"1. Ensure 'tensorboard' is installed (`pip install tensorboard`)\")\n",
        "    print(f\"2. In a SEPARATE terminal, navigate to the directory containing '{output_dir}'\")\n",
        "    print(f\"3. Run: tensorboard --logdir {output_dir}\")\n",
        "    print(f\"4. Open the URL provided (usually http://localhost:6006) in your browser.\")\n",
        "    print(f\"   (If running on a remote server, you might need SSH port forwarding: ssh -N -L 6006:localhost:6006 user@server)\")\n",
        "    print(\"\\nTraining logs will also appear below...\\n\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        trainer.train()\n",
        "        success = True\n",
        "    except Exception as e:\n",
        "        print(f\"\\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "        print(f\"            TRAINING ERROR            \")\n",
        "        print(f\"{e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\")\n",
        "        success = False\n",
        "\n",
        "    # --- Saving ---\n",
        "    final_save_dir = os.path.join(output_dir, \"final_model\")\n",
        "    checkpoint_save_dir = os.path.join(output_dir, \"checkpoint_on_error\")\n",
        "\n",
        "    if success:\n",
        "        print(\"\\nTraining completed successfully.\")\n",
        "        try:\n",
        "            print(f\"Saving final model and tokenizer to {final_save_dir}...\")\n",
        "            trainer.save_model(final_save_dir)\n",
        "            tokenizer.save_pretrained(final_save_dir)\n",
        "            print(\"Final model saved.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving final model: {e}\")\n",
        "    else:\n",
        "        print(\"\\nTraining failed or was interrupted.\")\n",
        "        try:\n",
        "            print(f\"Attempting to save checkpoint to {checkpoint_save_dir}...\")\n",
        "            trainer.save_model(checkpoint_save_dir)\n",
        "            tokenizer.save_pretrained(checkpoint_save_dir)\n",
        "            print(\"Checkpoint saved.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not save checkpoint: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Ensure the dataset file exists before starting\n",
        "    if not os.path.exists(\"expert_demos_batched_avg_q.csv\"):\n",
        "        print(\"ERROR: Dataset file 'expert_demos_batched_avg_q.csv' not found.\")\n",
        "        print(\"Please ensure the dataset is in the same directory as the script.\")\n",
        "    else:\n",
        "        train_gemma_on_frozen_lake_with_grpo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a05a82b1-069d-4d6b-b810-08aae118615a",
      "metadata": {
        "id": "a05a82b1-069d-4d6b-b810-08aae118615a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "transformers",
      "language": "python",
      "name": "transformers"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}